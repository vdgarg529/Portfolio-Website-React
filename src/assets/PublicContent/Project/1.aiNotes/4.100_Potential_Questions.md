### **1. Project Overview & Architecture (10 Questions)**

**1. What is the primary function of this application?**  
This is a secure, AI-powered note-taking API. It allows users to store notes, perform semantic searches across them, and receive AI-generated summaries. It implements a RAG (Retrieval-Augmented Generation) pattern for the query functionality.

**2. How would you classify the application's architecture?**  
It follows a modular, service-based architecture within a single FastAPI application. Key components (auth, DB, vector DB, LLM) are separated into distinct services for maintainability and separation of concerns.

**3. What are the main data storage components and their purposes?**  
SQLite (via SQLAlchemy) for structured, user-related data (emails, hashed passwords). ChromaDB, a vector database, for unstructured note data and their embeddings to enable semantic search.

**4. What is the key architectural pattern used for the search functionality?**  
Retrieval-Augmented Generation (RAG). User queries are used to retrieve relevant note chunks from the vector store (ChromaDB), which are then fed as context to a large language model (Gemini) to generate a coherent, context-aware summary.

**5. Why is a vector database like ChromaDB essential for this project?**  
It allows for semantic search, which is far more powerful than keyword matching. It finds notes with similar meaning to the query, even if they don't share exact keywords, by comparing numerical vector representations (embeddings) of the text.

**6. Walk me through the data flow for a "/notes/query" request.**

1. Authenticated request arrives. 2. Query text is converted to an embedding. 3. ChromaDB finds the most semantically similar note vectors. 4. The text of these notes is extracted. 5. The texts and original query are sent to the Gemini API. 6. Gemini's summary response is returned to the user.
    

**7. What are the major external dependencies and their roles?**  
FastAPI (web framework), SQLAlchemy (ORM), ChromaDB (vector store), Sentence-Transformers (local embedding model), Google Gemini API (LLM for summarization), and Passlib (password hashing).

**8. How does the application handle user data isolation?**  
Each user has a dedicated ChromaDB collection named `user_{user_id}_notes`. All note operations are scoped to this collection, ensuring users can only ever access their own data.

**9. What is the role of the `services/` directory?**  
It encapsulates all core business logic and external integrations (database, auth, AI, embeddings) away from the API route handlers, promoting modularity, testability, and single responsibility.

**10. If you were to add a frontend, how would it interact with this backend?**  
The frontend would be a Single Page Application (SPA) that makes HTTP requests to the defined FastAPI endpoints (e.g., POST `/notes/add`, GET `/notes/all`), using the JWT token for authentication in the `Authorization: Bearer <token>` header.

---

### **2. API Layer & `main.py` (10 Questions)**

**11. Why did you choose FastAPI for this project?**  
For its high performance (async support on Starlette), automatic interactive API documentation (Swagger UI/ReDoc), built-in data validation and serialization via Pydantic, and superior developer experience with type hints.

**12. What is the purpose of the `Depends(get_current_user)` parameter in your endpoints?**  
It's a FastAPI dependency that acts as a per-route middleware. It executes the `get_current_user` function to validate the JWT token from the request header before the main endpoint code runs, protecting the route.

**13. How do you handle potential blocking operations (like ChromaDB calls) in an async endpoint?**  
I use `await asyncio.to_thread()` to offload the synchronous, potentially blocking function (e.g., `query_notes`) to a separate thread, preventing it from blocking the main async event loop and maintaining server responsiveness.

**14. Explain the purpose of the CORS middleware configuration.**  
It configures Cross-Origin Resource Sharing to allow a web browser from one origin (e.g., `http://localhost:3000`) to access resources from this backend (`http://localhost:8000`). The current config (`allow_origins=["*"]`) is permissive for development but must be restricted to specific origins in production.

**15. What is the difference between the `POST /notes/query` and `GET /notes/all` endpoints?**  
`/notes/query` performs a _semantic search_: it finds notes _relevant_ to a query string using vector similarity. `/notes/all` performs a _simple retrieval_: it returns every single note for the user, in no particular order, with no filtering.

**16. How does the `POST /notes/add` endpoint ensure data quality?**  
It uses Pydantic's `NoteRequest` model for validation and includes a manual check: `if not request.text.strip():` to reject requests with empty or whitespace-only note text, preventing junk data from being stored.

**17. What is the function of the `@app.put("/notes/edit")` endpoint?**  
It allows a user to update the text content of an existing note. It takes a note ID and new text, validates the input, and calls the `edit_note` service function which updates the document text, its embedding, and the timestamp in ChromaDB.

**18. How does the PDF download endpoint work?**  
It uses the `reportlab` library to generate a PDF in memory (using `BytesIO`). It fetches all user notes, iterates through them, and writes the timestamp and text (with word wrapping) to the PDF canvas, which is then returned as a downloadable attachment.

**19. Why is the `GET /health` endpoint useful?**  
It provides a simple endpoint for health checks, often used by container orchestration systems (like Kubernetes), load balancers, or monitoring tools to verify that the application instance is running and responsive.

**20. How does error handling work in the endpoints?**  
Specific HTTPExceptions are raised for expected errors (e.g., 400 for empty query, 404 for note not found). Endpoints that call complex services are wrapped in try/except blocks to catch unexpected errors and return a 500 status code with a generic message to avoid leaking internal details.

---

### **3. Data Models & `models.py` (10 Questions)**

**21. What is the purpose of Pydantic models in this project?**  
They primarily define the schema for HTTP request bodies and response payloads. This provides automatic validation, serialization, and documentation, ensuring that incoming data is correct and outgoing data is well-formed.

**22. Why does `NoteMetadata` use `Field(default_factory=...)` instead of a direct default value?**  
Using `default_factory=lambda: str(uuid.uuid4())` ensures a new UUID is generated _each time_ an instance is created. A direct default value like `id: str = str(uuid.uuid4())` would be evaluated once at import time, meaning every instance would share the same ID.

**23. What is the difference between `RegisterRequest` and `LoginRequest`?**  
They are structurally identical but semantically different. Using separate models follows the principle of separation of concerns and allows for independent evolution (e.g., adding a `username` field to registration later wouldn't affect the login process).

**24. Why is there no `user_id` field in the `NoteRequest` model?**  
The user is already identified by the validated JWT token via the `get_current_user` dependency. Including it in the request body would be redundant and a security risk, as a malicious user could try to spoof another user's ID.

**25. How would you extend the `NoteMetadata` model to include a "title" for notes?**  
Add a field: `title: Optional[str] = Field(default=None, max_length=100)`. You would need to modify the `add_note` service and the frontend to handle this new field.

**26. What is the advantage of using Pydantic's built-in validation over manual checks in the endpoint?**  
It's declarative, consistent, and reduces boilerplate code. For example, you can enforce an email format with `email: EmailStr` or string length with `password: str = Field(..., min_length=8)`, and FastAPI will automatically return a 422 error for invalid data.

**27. Why is `EditNoteRequest` a separate model instead of reusing `NoteRequest`?**  
It requires an additional `note_id` field to identify which note to update. Reusing `NoteRequest` would make the `note_id` mandatory for creating notes, which is incorrect. This keeps the models specific to their use cases.

**28. How could you validate that a `timestamp` field in a request is a valid ISO format?**  
Pydantic has a built-in `datetime` type. You could define the field as `timestamp: datetime`, and Pydantic would automatically parse and validate any string that matches common datetime formats, including ISO 8601.

**29. What is the purpose of the `QueryRequest` model?**  
It defines the expected structure for the search endpoint's request body, ensuring that the incoming JSON contains a single `query` string field. This simplifies the endpoint code and provides clear documentation.

**30. If you wanted to add pagination to the `GET /notes/all` response, what models would you change?**  
You'd need a new Pydantic model for the response, e.g., `NoteListResponse`, containing `notes: List[NoteMetadata]`, `total_count: int`, `page: int`, and `page_size: int`. The endpoint would then return this model instead of a raw list.

---

### **4. Database & `services/db.py` (10 Questions)**

**31. What is the role of the `SessionLocal` object?**  
It's a session factory (created by `sessionmaker()`). Each call to `SessionLocal()` creates a new, unique database session object that represents a single transactional scope with the SQLite database.

**32. Explain the `Base = declarative_base()` line.**  
It creates a base class for all ORM models to inherit from. This base class maintains a catalog of all defined classes and tables, which is used by `Base.metadata.create_all()` to generate the corresponding database schema.

**33. Why is the database URL hardcoded as SQLite?**  
This is likely for development simplicity. In a production environment, this would be injected via an environment variable (e.g., `DATABASE_URL`) to allow easy switching to more powerful databases like PostgreSQL without code changes.

**34. How is the database schema created?**  
The line `Base.metadata.create_all(bind=engine)` in `main.py` runs on application startup. It queries the metadata associated with the `Base` class and issues CREATE TABLE statements for any tables that don't already exist in the database.

**35. What is the purpose of the `User` model's `__tablename__` attribute?**  
It explicitly defines the name of the database table that will be created for the `User` model. Without it, SQLAlchemy would automatically generate a table name (like `user`), which can sometimes lead to conflicts with reserved keywords.

**36. How are database sessions typically managed in a FastAPI application?**  
Using a dependency that creates a session for each request and closes it afterwards. This pattern is not fully shown here but would involve a middleware or dependency that yields a session and ensures it's closed in a `finally` block, preventing connection leaks.

**37. Why is the `id` field a `String` type instead of an `Integer`?**  
It's storing a UUID (e.g., `str(uuid.uuid4())`). UUIDs are preferable for distributed systems or when exposing IDs publicly, as they are globally unique and non-sequential, avoiding enumeration attacks.

**38. What does the `unique=True` constraint on the `email` field enforce?**  
It creates a unique index in the database, preventing two users from registering with the same email address. The application code also checks for this to return a friendly 400 error, but the database constraint is the ultimate guarantee of data integrity.

**39. What is the benefit of using `created_at` with a default function?**  
It automatically records the timestamp of when a user was created without the application code having to explicitly provide it. This is useful for auditing and analytics.

**40. If you needed to add a "last_login" feature, how would you modify the `User` model?**  
Add a new column: `last_login = Column(DateTime, nullable=True)`. You would then update the login logic to set `user.last_login = datetime.utcnow()` and commit the session upon successful authentication.

---

### **5. Authentication & `services/auth.py` (10 Questions)**

**41. Explain the OAuth2PasswordBearer flow used in this project.**  
It's a standard flow where a user submits a username and password (via `POST /login`). The server verifies them and returns an access token. The client then sends this token in the `Authorization: Bearer <token>` header for all subsequent requests to protected endpoints.

**42. How do you securely verify a user's password during login?**  
Using `passlib`'s `verify_password(plain_password, hashed_password)` function. It hashes the provided plaintext password using the same algorithm and parameters (salt, work factor) as the stored hash and compares the results in a constant-time manner to prevent timing attacks.

**43. What information is encoded inside the JWT access token?**  
The token's payload contains a "sub" (subject) claim, which holds the user's ID. It also contains an "exp" (expiration) claim set to 30 minutes after creation. The token is signed with the SECRET_KEY to prevent tampering.

**44. What is the critical security flaw with the current `SECRET_KEY` setup?**  
It's hardcoded as `"your-secret-key"`. In production, this must be a long, random, and secret string stored in an environment variable. A hardcoded or weak secret key allows attackers to easily forge their own valid JWTs.

**45. Why is the `get_current_user` function `async`?**  
Because the `OAuth2PasswordBearer` dependency is asynchronous. Even though the function body uses synchronous `jwt.decode`, the function itself must be `async` to be compatible with FastAPI's async dependency injection system.

**46. What is the purpose of the `ALGORITHM` variable being set to "HS256"?**  
HS256 (HMAC with SHA-256) is the algorithm used to sign and verify the JWT. It's a symmetric algorithm, meaning the same secret key is used to both create and verify the signature.

**47. How does the `create_access_token` function handle token expiration?**  
It calculates an expiry time (`expire`) by adding a `timedelta` (defaulting to 15 minutes if none provided) to the current UTC time. This timestamp is then added to the payload that gets encoded into the JWT.

**48. What happens if an expired JWT is sent to a protected endpoint?**  
The `jwt.decode()` function will fail with a `JWTError` because the current time is past the "exp" claim. This error is caught, and the `get_current_user` dependency raises an HTTP 401 "Invalid token" exception.

**49. How could you implement token revocation or a "logout" feature?**  
You would need to move from a stateless JWT to a stateful mechanism. This typically involves storing a blacklist of revoked tokens in a fast, ephemeral store like Redis and checking it on every request, which adds complexity.

**50. What is the "work factor" for the bcrypt algorithm, and why is it important?**  
The work factor (or cost factor) determines how computationally expensive the hashing process is (e.g., `rounds=12`). It's crucial for security because it slows down both legitimate login attempts and, more importantly, brute-force attacks by an attacker who has obtained the password hash database.

---

### **6. Embeddings & `services/embedding_service.py` (10 Questions)**

**51. Why is the embedding model loaded once as a global variable?**  
Loading the model (`model = SentenceTransformer(...)`) is computationally expensive and memory-intensive. Loading it once at startup and reusing it across all requests (it's thread-safe) provides massive performance benefits and avoids constant disk I/O.

**52. What model are you using, and what are its characteristics?**  
The `all-MiniLM-L6-v2` model. It's a great general-purpose model that maps text to a 384-dimensional vector space. It's a trade-off: it's fast, has a small footprint, and performs well for its size, though larger models might yield better accuracy.

**53. What is the output of the `get_embedding` function?**  
It returns a NumPy array of 384 floating-point numbers (for this model) representing the semantic meaning of the input text in a high-dimensional vector space. The `.tolist()` method in the caller converts this to a standard Python list for JSON serialization.

**54. Why did you choose a local embedding model over an API-based one (like OpenAI's)?**  
Primarily for data privacy, cost reduction, and latency. User notes never leave the server. There are no ongoing API costs, and inference is very fast after the initial model load, avoiding network latency for every embedding request.

**55. What is a potential downside of using this specific local model?**  
The embedding quality, while good, may be lower than that of a larger, state-of-the-art API-based model (e.g., OpenAI's text-embedding-3-large). The 384 dimensions may not capture semantic nuance as well as a higher-dimensional embedding.

**56. How does the embedding process affect the overall performance of adding a note?**  
It's the most computationally expensive part of the `add_note` operation. The time taken is proportional to the length of the text. This is why it's done synchronously within the endpoint's thread pool executor.

**57. Could this service be swapped for a different embedding model? How?**  
Yes, easily. The `get_embedding` function provides a clean interface. To switch to OpenAI, you'd replace the function body with an HTTP call to their embedding API and ensure the output is a list of floats of the expected dimension.

**58. What is the significance of the vector dimension (384) for ChromaDB?**  
All embeddings added to a ChromaDB collection must have the same dimension. This value defines the structure of the vector space that ChromaDB will perform operations in (e.g., cosine similarity calculations).

**59. What might happen if you tried to query a collection with an embedding of a different dimension?**  
ChromaDB would likely raise an error. The query embedding's dimension must match the dimension of the embeddings already stored in the collection for the similarity calculation to be mathematically valid.

**60. How would you handle very long notes that exceed the model's context length?**  
You would implement a chunking strategy before generating embeddings. Split the long note into smaller, overlapping chunks (e.g., 512 tokens each), generate an embedding for each chunk, and store them all in ChromaDB with metadata linking them to the original note.

---

### **7. Vector Database & `services/chroma_service.py` (20 Questions)**

**61. What is the difference between `PersistentClient` and `EphemeralClient`?**  
`PersistentClient` saves the database to disk (in the `./chroma_db` path), so data persists between application restarts. `EphemeralClient` holds everything in memory and is lost when the client is closed. Persistent is required for a production application.

**62. Explain the `get_user_collection` function's purpose.**  
It implements a data isolation pattern. It generates a unique collection name for each user (`user_{user_id}_notes`) and uses `get_or_create_collection` to fetch it. This ensures all operations for a user are scoped to their personal collection.

**63. Why is `embedding_function=None` explicitly set when getting the collection?**  
This is a critical fix. By default, Chroma might use its own embedding function. By setting it to `None`, we explicitly tell Chroma that we are providing our own pre-computed embeddings, which is what the `add_note` function does.

**64. Walk me through the exact steps of the `add_note` function.**

1. Get the user's collection. 2. Generate an embedding vector for the note text. 3. Create a `NoteMetadata` object (which auto-generates an ID and timestamp). 4. Call `collection.add()` with the `documents`, `embeddings`, `metadatas`, and `ids`.
    

**65. How does `query_notes` find relevant notes?**  
It generates an embedding for the query text. It then asks ChromaDB to find the `n_results` (which is set to `collection.count()`) vectors in the user's collection that have the highest cosine similarity to the query vector. It then filters results below a similarity threshold (0.5).

**66. Why filter results by a similarity score threshold?**  
To improve the quality of the summary. Notes with very low similarity (e.g., < 0.5) are likely not relevant to the query and would just add noise to the context sent to the LLM, potentially leading to a worse or hallucinated summary.

**67. What does the `has_notes` function do?**  
It provides a simple way to check if a user has any notes stored at all, without the overhead of fetching all the data. It does this by checking if the `count()` of documents in their collection is greater than zero.

**68. How does `get_all_notes_chroma` differ from a query?**  
It uses `collection.get()` instead of `collection.query()`. `get()` retrieves all items in the collection by their ID, ignoring the vector index. It's used for operations where you need all data (like PDF export), not a semantic search.

**69. Explain the `delete_note` function.**  
It's straightforward. It takes a user ID and a note ID, gets the user's collection, and calls `collection.delete(ids=[note_id])`. ChromaDB handles the removal of the corresponding document, embedding, and metadata.

**70. What happens in the `edit_note` function?**

1. It fetches the existing note by ID. 2. It creates a copy of the old metadata. 3. It updates the `text` and `timestamp` in the new metadata. 4. It generates a _new embedding_ for the updated text. 5. It calls `collection.update()` to replace the old document, embedding, and metadata.
    

**71. Why is it necessary to generate a new embedding when editing a note?**  
Because the embedding is a mathematical representation of the text's meaning. Changing the text ("my cat" -> "my dog") completely changes its semantic meaning and, therefore, its vector representation. The old embedding would be invalid for search.

**72. What is the "hnsw:space": "cosine" metadata for?**  
It configures the distance function used for similarity search in the Chroma collection. "cosine" means it will use cosine similarity, which measures the angle between vectors and is a common, effective metric for text embeddings.

**73. What is a potential performance issue with `query_notes` as implemented?**  
It sets `n_results=collection.count()` to get all results before filtering. For a user with a very large number of notes (e.g., 10,000), this could be inefficient. It would be better to let Chroma do the filtering by using `where` clauses or a more appropriate `n_results` value.

**74. How does ChromaDB handle persistence with the `PersistentClient`?**  
It automatically persists any changes (add, update, delete) to the specified directory on disk. The data is loaded back into memory when the application restarts and the `PersistentClient` connects to the same path.

**75. What is the structure of the data returned by `collection.query()`?**  
It returns a dictionary with keys like `'ids'`, `'embeddings'`, `'documents'`, `'metadatas'`, and `'distances'`. Each value is a list of lists, where the first list corresponds to the first query embedding (we only use one), and the inner lists contain the results.

**76. What does the `distances` array represent in the query results?**  
It represents the calculated distance between the query vector and each result vector. For cosine similarity, a lower distance means the vectors are closer together (more similar). The code converts this to a similarity score with `1 - dist`.

**77. How could you add filtering by date to a query?**  
ChromaDB supports metadata filtering. You would add a `where` filter to the `query` call: e.g., `.where({"timestamp": {"$gte": "2024-01-01"}})`. This would only return notes whose timestamp metadata is on or after Jan 1, 2024.

**78. What is a key advantage of using ChromaDB's built-in metadata storage?**  
It allows you to store and filter by structured data (like IDs, timestamps, sources) alongside your vectors, without needing a separate database. This is more efficient than storing this data only in the document text.

**79. If ChromaDB was down, what would be the impact?**  
All note-related operations (add, query, edit, delete, list all) would fail. The user authentication (`/login`, `/register`) would still work, as it relies on SQLite. The application would be largely unusable for its core functionality.

**80. How might you improve the error handling in these service functions?**  
The current functions often return `False` on failure. You could instead raise specific, custom exceptions (e.g., `NoteNotFoundError`, `ChromaConnectionError`) that the API layer could catch and convert into more descriptive HTTP error responses.

---

### **8. LLM Integration & `services/llm_utils.py` (10 Questions)**

**81. What is the role of the `generate_summary` function?**  
It acts as the "Generation" component in the RAG pipeline. It takes the context (relevant notes retrieved from ChromaDB) and the original user query, constructs a prompt for the LLM, and returns a coherent, summarized answer based solely on that context.

**82. Why is the `combined_text` constructed with separators ("--- NOTE ---")?**  
This prompt engineering technique helps the LLM clearly distinguish between different context documents. It prevents the model from getting confused and treating multiple notes as one continuous block of text, leading to a higher quality summary.

**83. What is the purpose of the system prompt ("You are an expert note summarizer...")?**  
It uses the system message to prime the model and set its role, behavior, and goal before giving it the context and query. This steering leads to more focused and relevant outputs that align with the user's expectation of a summary.

**84. Why are the generationConfig parameters `temperature=0.3` and `maxOutputTokens=512` used?**  
A low temperature (0.3) makes the model output more deterministic and focused, reducing the chance of irrelevant or hallucinated content. The token limit prevents overly verbose responses and helps control API costs.

**85. How does this implementation mitigate LLM hallucination?**  
By using the RAG pattern. The instruction "Focus on connecting insights across notes" and the structure of the prompt strongly encourage the model to base its answer strictly on the provided context (the user's notes) rather than on its internal knowledge.

**86. What is the key difference between the two `generate_summary` implementations shown?**  
The second one has more robust error handling. It includes an additional try/except block to catch `KeyError` or `IndexError` when parsing the Gemini API response, preventing crashes if the API returns an unexpected JSON structure.

**87. Why use an async HTTP client (`httpx.AsyncClient`)?**  
To avoid blocking the entire FastAPI event loop while waiting for the Gemini API to respond. This allows the server to handle other incoming requests concurrently during the (potentially slow) network call to the external API.

**88. What happens if the Gemini API returns a non-200 status code?**  
The `response.raise_for_status()` call will trigger an `HTTPStatusError` exception. This is caught, logged, and the function returns a user-friendly error message instead of crashing the application.

**89. How could you implement a fallback if the Gemini API is unavailable?**  
You could catch the request exception and return a simple, non-AI summary instead, such as concatenating the first few lines of each retrieved note. This provides a degraded but still functional user experience.

**90. What are the limitations of the current prompt design?**  
It's a static, hardcoded string. A more advanced system would use a templating engine (e.g., Jinja2) to allow for more dynamic and flexible prompt construction, making it easier to experiment with and improve different instructions.

---

### **9. System Design & Scalability (10 Questions)**

**91. How would you containerize this application for deployment?**  
I would create a multi-stage Dockerfile. Stage 1: Use a Python image to install dependencies and download the embedding model. Stage 2: Copy the application code and installed packages into a smaller runtime image. I'd use `docker-compose` to define the service and its volumes for `chroma_db` and `users.db`.

**92. The SQLite database is a bottleneck. How would you fix it?**  
I would migrate to a client-server database like PostgreSQL. This involves: 1. Changing the `SQLALCHEMY_DATABASE_URL`. 2. Using a different dialect in SQLAlchemy. 3. Running the database as a separate, scalable container or using a managed cloud service (e.g., AWS RDS, Google Cloud SQL).

**93. How would you scale the FastAPI application itself?**  
I would run multiple instances of the application behind a load balancer (e.g., Nginx or a cloud load balancer). The application is mostly stateless (except for the in-memory embedding model), making it easy to scale horizontally. A process manager like Gunicorn with Uvicorn workers would be used in each container.

**94. The embedding model is memory-heavy. How does this impact scaling?**  
Each application instance loads its own copy of the model into memory. This significantly increases the memory footprint per instance, making vertical scaling (bigger machines) more expensive and limiting the number of containers you can run on a single node.

**95. How could you mitigate the embedding model's memory issue?**  
I would create a separate, dedicated embedding service. The main API workers would make HTTP/RPC calls to this service to get embeddings. This allows the embedding model to be loaded only once and the API workers to be lightweight, simplifying scaling for both components independently.

**96. ChromaDB in persistent mode doesn't scale well. What's the alternative?**  
For a production system, I would replace the local ChromaDB with a managed, scalable vector database service like Pinecone, Weaviate, or Qdrant. These are designed for high availability, large datasets, and distributed performance, which ChromaDB in its current form is not.

**97. How would you monitor this application in production?**  
I would implement: 1. Application logging (e.g., using `structlog` or `loguru`) with levels. 2. Export metrics (request latency, error rates) to Prometheus. 3. Set up health checks. 4. Use distributed tracing (e.g., OpenTelemetry) to track requests across services. 5. Configure alerts based on these metrics.

**98. How could you improve the performance for frequent, repeated queries?**  
I would introduce a caching layer using Redis. The key would be a hash of the user ID and the query text, and the value would be the API response (summary + results). This would drastically reduce response times and load on both ChromaDB and the Gemini API for popular queries.

**99. What is a major security consideration for the PDF download feature?**  
It's vulnerable to a Denial-of-Service (DoS) attack. A user with a very large number of notes could generate a massive PDF, consuming significant server CPU and memory resources. You should implement pagination or a hard limit on the number of notes included in a single PDF.

**100. How would you plan for database schema migrations in the future?**  
I would integrate a dedicated migration tool like Alembic. This would allow me to script changes to the SQLAlchemy models (e.g., adding a `last_login` column) and apply those changes to the production database in a controlled, versioned, and reversible way.

New chat