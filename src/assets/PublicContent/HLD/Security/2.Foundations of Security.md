# 1. Foundations of Security → 1.1 Basic Security Principles

## CIA Triad
**What it is:** Three core properties every system should protect.
- **Confidentiality** — Only authorized parties can read data.  
  **Examples:** Encrypting database columns with AES-256; using TLS for APIs; masking PANs in logs.  
  **Design applications:**  
  - Data classification drives control strength (PII vs telemetry).  
  - Choose transport security (TLS 1.2+) and at-rest encryption (KMS-managed keys).  
  - Access paths: app → API → DB should all enforce authZ; never allow direct DB access from the internet.  
  **Pitfalls:** Encrypting data but storing keys in source code; public S3/Blob buckets; broad read permissions to “analytics” roles.  
  **Best practices:** Key management (KMS/HSM), least-privileged IAM, secrets in a vault, data minimization, field-level encryption where warranted.

- **Integrity** — Data isn’t altered without detection.  
  **Examples:** HMACs on webhook payloads; signed JWTs; database checksums; Git commit signatures.  
  **Design applications:**  
  - Use hashing + MAC (HMAC-SHA-256) for messages; digital signatures for non-repudiation.  
  - Protect update paths (e.g., only services with “write” role can mutate orders).  
  **Pitfalls:** Using hashes without a secret (plain SHA-256) to “prove authenticity”; accepting unsigned webhooks.  
  **Best practices:** Authenticated encryption (AEAD like AES-GCM), strong hashing (SHA-256/512), digital signatures (Ed25519/ECDSA) for provenance, write-audit logs.

- **Availability** — Systems are usable when needed.  
  **Examples:** Multi-AZ databases; autoscaling; rate-limits/WAF; circuit breakers; S3 versioning + lifecycle.  
  **Design applications:**  
  - DR strategy (RTO/RPO), redundancy (N+1), backpressure, graceful degradation.  
  **Pitfalls:** Single region deployments; stateful services with no failover; ignoring dependency SLAs.  
  **Best practices:** HA & failover testing, load shedding, retries with jitter, idempotent APIs, chaos drills.

**Trade-offs:** Controls can conflict (e.g., strict rate limits protect availability but can reduce usability; envelope encryption boosts confidentiality but adds latency). State your trade-offs explicitly in the HLD.

---

## Principle of Least Privilege (PoLP)
**What it is:** Every entity (user, service, job) gets the minimum permissions needed, only for the time needed.

**Examples & patterns in design:**
- **Scoped tokens:** Access tokens limited to specific resources/scopes (e.g., `read:orders`).  
- **Role segmentation:** Separate “read-replica access” role from “writer” role for analytics vs app.  
- **JIT access:** Human admin access is time-boxed via PAM; emergency “break-glass” with auditing.  
- **Network egress control:** Services can only talk to approved endpoints.

**Pitfalls:** Wildcards like `s3:*`; long-lived keys; sharing service accounts; privilege creep over time.  
**Best practices:** Default-deny policies, short-lived creds (OIDC-federated or STS), group-based RBAC, periodic access reviews, policy-as-code with tests, per-env isolation (dev/stage/prod).

---

## Defense in Depth
**What it is:** Multiple, independent layers of control so a single failure doesn’t cause a breach.

**Layered example for a web/microservices system:**
1. **Edge:** DNS security + CDN/WAF (bot/DDoS mitigation, basic OWASP rules).  
2. **Transport:** TLS 1.2+ everywhere, HSTS, modern ciphers; mTLS for service-to-service.  
3. **Application:** Centralized authN/Z, input validation, output encoding, CSRF/XSS defenses, rate limits.  
4. **Data:** Encryption at rest with KMS, row/column-level controls, tokenization of high-risk fields.  
5. **Platform:** Container hardening, read-only roots, minimal base images, signed artifacts, SBOM.  
6. **Network:** Segmentation, zero-trust access, private subnets, egress allow-lists.  
7. **Ops:** Logging, SIEM rules, anomaly detection, backups with restore drills, secrets management.  

**Pitfalls:** Assuming one strong control (e.g., WAF) is enough; layers that depend on the same root secret; unmanaged complexity that creates misconfigurations.  
**Best practices:** Independent layers, blast-radius reduction, “assume breach,” continuous validation (policy tests, security unit tests).

---

## Threat Modeling (STRIDE, DREAD, Attack Trees)
**Why:** Find risks early, bake mitigations into the design, and document residual risk.

### STRIDE (what can go wrong)
- **S: Spoofing identity** → strong authentication, mTLS, signed requests.  
- **T: Tampering with data** → integrity checks (HMAC/signatures), WORM logs, RBAC on writes.  
- **R: Repudiation** (denying actions) → non-repudiation via signatures, immutable audit logs, time sync.  
- **I: Information disclosure** → data classification, encryption, masking, least privilege.  
- **D: Denial of service** → rate limits, autoscaling, isolation, bulkheads.  
- **E: Elevation of privilege** → strict authZ, input validation, sandboxing, patching.

**How to run it:** Draw a basic DFD (actors, processes, data stores, trust boundaries). For each flow crossing a trust boundary, walk STRIDE and note mitigations.

### DREAD (risk rating)
- **What it was:** Qualitative scoring of risk (Damage, Reproducibility, Exploitability, Affected users, Discoverability).  
- **Reality:** Considered inconsistent/subjective; many teams now use **OWASP Risk Rating**, **CVSS**, or simple **Likelihood × Impact** matrices.  
- **Actionable approach:** Use Likelihood (Low/Med/High) × Impact (Low/Med/High), justify briefly, and tie to SLAs for remediation.

### Attack Trees
- **What:** A tree with attacker *goal* at the root (e.g., “Exfiltrate customer PII”), decomposed into **AND/OR** paths.  
- **Use:** Helps spot alternative routes (e.g., phishing SSO vs exploiting SSRF to reach metadata service vs stealing backups).  
- **Design output:** Place controls along multiple branches (FIDO2 for phishing resistance, IMDSv2 and egress controls for SSRF, backup encryption + separate keys).

**Threat modeling deliverables for an HLD:**
- System context & trust boundaries diagram.  
- Asset inventory & data classification.  
- STRIDE findings mapped to controls.  
- Risk ratings with owners, due dates, and residual risk notes.  
- Assumptions & non-goals (so reviewers can challenge them).

---

## Quick comparisons you’ll make in HLDs
- **Integrity control:** HMAC (shared key, fast) vs digital signatures (asymmetric, non-repudiation).  
- **Protecting PII:** Encryption (reversible) vs hashing (one-way) vs tokenization (replace with surrogate).  
- **Availability strategy:** Active-active (lower RTO, higher cost/complexity) vs active-passive (cheaper, higher RTO).

---

## Common red flags in designs (call these out early)
- Publicly reachable data stores; flat VPCs with no segmentation.  
- Long-lived user/service credentials; secrets in env vars checked into CI logs.  
- “We’ll add security later” or “internal service—no auth needed.”  
- No DR plan; backups in the same blast radius; untested restores.  
- Logging PII or tokens; accepting unsigned webhooks; missing rate limits.

---

## Interview-style prompts to practice
- “Pick a feature (payments, file upload, webhooks). Walk me through CIA and threat model it.”  
- “Choose a mitigation for SSRF in a microservice pulling from URLs.”  
- “When do you pick HMAC vs ECDSA for message integrity, and why?”
